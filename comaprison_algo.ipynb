{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/DataQuests/Pandas/raw/master/with_cpep.csv'\n",
    "df = pandas.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"withRatio.csv\")\n",
    "#X = df[['age_bl', 'fma', 'ffma', 'GENDER', 'B meanumb', 'B temp', 'WhtR', 'AGE_WhtR']]\n",
    "X = df[['ffma', 'GENDER', 'B meanwst', 'B meanumb', '1 clinwt', 'AGE_WhtR']]\n",
    "y = df['cat ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deidnum</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>age_bl</th>\n",
       "      <th>age_3</th>\n",
       "      <th>age_6</th>\n",
       "      <th>age_9</th>\n",
       "      <th>age_12</th>\n",
       "      <th>age_18</th>\n",
       "      <th>age_24</th>\n",
       "      <th>fma</th>\n",
       "      <th>...</th>\n",
       "      <th>12 WhtR</th>\n",
       "      <th>18 WhtR</th>\n",
       "      <th>24 WhtR</th>\n",
       "      <th>WhtR</th>\n",
       "      <th>WhtR 6</th>\n",
       "      <th>WhtR 12</th>\n",
       "      <th>WhtR 18</th>\n",
       "      <th>WhtR 24</th>\n",
       "      <th>AGE/BMI</th>\n",
       "      <th>AGE_WhtR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1057</td>\n",
       "      <td>0</td>\n",
       "      <td>42.01</td>\n",
       "      <td>42.43</td>\n",
       "      <td>42.68</td>\n",
       "      <td>42.93</td>\n",
       "      <td>43.18</td>\n",
       "      <td>43.68</td>\n",
       "      <td>44.18</td>\n",
       "      <td>18.44968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477481</td>\n",
       "      <td>0.463935</td>\n",
       "      <td>0.430071</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.757850</td>\n",
       "      <td>23.473256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081</td>\n",
       "      <td>0</td>\n",
       "      <td>39.21</td>\n",
       "      <td>39.60</td>\n",
       "      <td>39.85</td>\n",
       "      <td>40.10</td>\n",
       "      <td>40.35</td>\n",
       "      <td>40.85</td>\n",
       "      <td>41.35</td>\n",
       "      <td>25.62774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486815</td>\n",
       "      <td>0.475225</td>\n",
       "      <td>0.472327</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.564905</td>\n",
       "      <td>20.252632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2230</td>\n",
       "      <td>0</td>\n",
       "      <td>40.56</td>\n",
       "      <td>40.92</td>\n",
       "      <td>41.17</td>\n",
       "      <td>41.42</td>\n",
       "      <td>41.67</td>\n",
       "      <td>42.17</td>\n",
       "      <td>42.67</td>\n",
       "      <td>26.45387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467144</td>\n",
       "      <td>0.473373</td>\n",
       "      <td>0.445344</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.530727</td>\n",
       "      <td>20.147369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2355</td>\n",
       "      <td>0</td>\n",
       "      <td>26.83</td>\n",
       "      <td>27.30</td>\n",
       "      <td>27.55</td>\n",
       "      <td>27.80</td>\n",
       "      <td>28.05</td>\n",
       "      <td>28.55</td>\n",
       "      <td>29.05</td>\n",
       "      <td>30.86636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489784</td>\n",
       "      <td>0.470252</td>\n",
       "      <td>0.509315</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.991849</td>\n",
       "      <td>14.007541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2373</td>\n",
       "      <td>0</td>\n",
       "      <td>25.79</td>\n",
       "      <td>26.18</td>\n",
       "      <td>26.43</td>\n",
       "      <td>26.68</td>\n",
       "      <td>26.93</td>\n",
       "      <td>27.43</td>\n",
       "      <td>27.93</td>\n",
       "      <td>22.65525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498423</td>\n",
       "      <td>0.503155</td>\n",
       "      <td>0.504732</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.013932</td>\n",
       "      <td>13.118728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   deidnum  GENDER  age_bl  age_3  age_6  age_9  age_12  age_18  age_24  \\\n",
       "0     1057       0   42.01  42.43  42.68  42.93   43.18   43.68   44.18   \n",
       "1     2081       0   39.21  39.60  39.85  40.10   40.35   40.85   41.35   \n",
       "2     2230       0   40.56  40.92  41.17  41.42   41.67   42.17   42.67   \n",
       "3     2355       0   26.83  27.30  27.55  27.80   28.05   28.55   29.05   \n",
       "4     2373       0   25.79  26.18  26.43  26.68   26.93   27.43   27.93   \n",
       "\n",
       "        fma  ...   12 WhtR   18 WhtR   24 WhtR  WhtR  WhtR 6  WhtR 12  \\\n",
       "0  18.44968  ...  0.477481  0.463935  0.430071     2       1        1   \n",
       "1  25.62774  ...  0.486815  0.475225  0.472327     2       1        2   \n",
       "2  26.45387  ...  0.467144  0.473373  0.445344     2       1        1   \n",
       "3  30.86636  ...  0.489784  0.470252  0.509315     2       1        2   \n",
       "4  22.65525  ...  0.498423  0.503155  0.504732     2       2        2   \n",
       "\n",
       "   WhtR 18  WhtR 24   AGE/BMI   AGE_WhtR  \n",
       "0        1        1  1.757850  23.473256  \n",
       "1        1        1  1.564905  20.252632  \n",
       "2        1        1  1.530727  20.147369  \n",
       "3        1        2  0.991849  14.007541  \n",
       "4        2        2  1.013932  13.118728  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 141)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE features\n",
    "col = ['age_bl', 'fma', 'ffma', 'GENDER', 'B meanumb', 'B meanwst', 'B temp', 'WhtR', 'AGE_WhtR', 'BMI']\n",
    "#cross validated RFE features (works)\n",
    "#col = ['age_bl', 'fma', 'ffma', 'GENDER', 'B meanumb', 'B temp', 'WhtR', 'AGE_WhtR']\n",
    "#correlation mapping\n",
    "#col = ['ffma', 'GENDER', 'B meanwst', 'B meanumb', '1 clinwt', 'AGE_WhtR']\n",
    "#Phik correlation mapping\n",
    "#col = ['B meanwst', 'B meanumb', 'bmi', 'ffma', 'GENDER']\n",
    "#univariate feature\n",
    "#col = ['ffma', 'GENDER', 'B meanwst', 'B meanumb', '1 clinwt', 'AGE_WhtR', 'B meansbp', 'age_bl', 'B meanbp', 'fma']\n",
    "#Extra Tree classifiers\n",
    "#col = ['GENDER', 'fma', 'B temp', 'B meanumb', 'B pulse', 'ffma', 'B meandbp', '1 clinwt', 'AGE_WhtR', 'B meanwst']\n",
    "X1 = df[col] \n",
    "y1 = df['cat ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>fma</th>\n",
       "      <th>B temp</th>\n",
       "      <th>B meanumb</th>\n",
       "      <th>B pulse</th>\n",
       "      <th>ffma</th>\n",
       "      <th>B meandbp</th>\n",
       "      <th>1 clinwt</th>\n",
       "      <th>AGE_WhtR</th>\n",
       "      <th>B meanwst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18.44968</td>\n",
       "      <td>36.80</td>\n",
       "      <td>82.500</td>\n",
       "      <td>64.5</td>\n",
       "      <td>33.20032</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>48.60</td>\n",
       "      <td>23.473256</td>\n",
       "      <td>72.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>25.62774</td>\n",
       "      <td>36.70</td>\n",
       "      <td>89.125</td>\n",
       "      <td>61.0</td>\n",
       "      <td>48.72226</td>\n",
       "      <td>64.166667</td>\n",
       "      <td>71.60</td>\n",
       "      <td>20.252632</td>\n",
       "      <td>78.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>26.45387</td>\n",
       "      <td>36.60</td>\n",
       "      <td>79.750</td>\n",
       "      <td>73.0</td>\n",
       "      <td>41.09613</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>63.15</td>\n",
       "      <td>20.147369</td>\n",
       "      <td>75.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>30.86636</td>\n",
       "      <td>36.85</td>\n",
       "      <td>86.875</td>\n",
       "      <td>82.0</td>\n",
       "      <td>43.43364</td>\n",
       "      <td>74.833333</td>\n",
       "      <td>71.70</td>\n",
       "      <td>14.007541</td>\n",
       "      <td>75.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.65525</td>\n",
       "      <td>36.70</td>\n",
       "      <td>80.625</td>\n",
       "      <td>65.0</td>\n",
       "      <td>41.19475</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>61.30</td>\n",
       "      <td>13.118728</td>\n",
       "      <td>75.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>28.67226</td>\n",
       "      <td>36.85</td>\n",
       "      <td>91.625</td>\n",
       "      <td>68.0</td>\n",
       "      <td>45.57774</td>\n",
       "      <td>65.166667</td>\n",
       "      <td>71.70</td>\n",
       "      <td>24.361014</td>\n",
       "      <td>82.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>22.35399</td>\n",
       "      <td>36.55</td>\n",
       "      <td>87.375</td>\n",
       "      <td>67.5</td>\n",
       "      <td>45.39601</td>\n",
       "      <td>71.666667</td>\n",
       "      <td>66.75</td>\n",
       "      <td>18.707309</td>\n",
       "      <td>78.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>16.03920</td>\n",
       "      <td>37.20</td>\n",
       "      <td>82.800</td>\n",
       "      <td>59.5</td>\n",
       "      <td>54.61080</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>66.30</td>\n",
       "      <td>17.772606</td>\n",
       "      <td>81.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>32.44627</td>\n",
       "      <td>36.75</td>\n",
       "      <td>84.375</td>\n",
       "      <td>79.0</td>\n",
       "      <td>39.75373</td>\n",
       "      <td>80.666667</td>\n",
       "      <td>70.30</td>\n",
       "      <td>23.212059</td>\n",
       "      <td>80.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>16.93972</td>\n",
       "      <td>36.60</td>\n",
       "      <td>65.750</td>\n",
       "      <td>64.0</td>\n",
       "      <td>39.38528</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>52.80</td>\n",
       "      <td>16.568048</td>\n",
       "      <td>64.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>19.89859</td>\n",
       "      <td>36.55</td>\n",
       "      <td>82.250</td>\n",
       "      <td>54.0</td>\n",
       "      <td>45.15141</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>61.10</td>\n",
       "      <td>16.856245</td>\n",
       "      <td>74.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>20.75006</td>\n",
       "      <td>36.30</td>\n",
       "      <td>90.625</td>\n",
       "      <td>67.0</td>\n",
       "      <td>39.82494</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>58.20</td>\n",
       "      <td>22.262512</td>\n",
       "      <td>82.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>29.01078</td>\n",
       "      <td>36.85</td>\n",
       "      <td>86.375</td>\n",
       "      <td>73.0</td>\n",
       "      <td>44.93922</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>72.75</td>\n",
       "      <td>12.658328</td>\n",
       "      <td>74.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>19.23703</td>\n",
       "      <td>36.55</td>\n",
       "      <td>90.875</td>\n",
       "      <td>57.0</td>\n",
       "      <td>56.93797</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>71.90</td>\n",
       "      <td>23.081703</td>\n",
       "      <td>86.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>28.56019</td>\n",
       "      <td>36.85</td>\n",
       "      <td>91.250</td>\n",
       "      <td>61.0</td>\n",
       "      <td>44.23981</td>\n",
       "      <td>66.166667</td>\n",
       "      <td>67.60</td>\n",
       "      <td>23.858505</td>\n",
       "      <td>85.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>23.34864</td>\n",
       "      <td>36.55</td>\n",
       "      <td>91.375</td>\n",
       "      <td>51.0</td>\n",
       "      <td>55.80136</td>\n",
       "      <td>71.166667</td>\n",
       "      <td>75.90</td>\n",
       "      <td>23.592797</td>\n",
       "      <td>88.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>23.45835</td>\n",
       "      <td>37.15</td>\n",
       "      <td>85.125</td>\n",
       "      <td>60.5</td>\n",
       "      <td>40.49165</td>\n",
       "      <td>77.333333</td>\n",
       "      <td>62.75</td>\n",
       "      <td>22.657431</td>\n",
       "      <td>76.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>22.77217</td>\n",
       "      <td>37.05</td>\n",
       "      <td>90.375</td>\n",
       "      <td>69.5</td>\n",
       "      <td>45.05283</td>\n",
       "      <td>66.833333</td>\n",
       "      <td>65.15</td>\n",
       "      <td>22.654802</td>\n",
       "      <td>85.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>19.87719</td>\n",
       "      <td>36.85</td>\n",
       "      <td>79.000</td>\n",
       "      <td>61.5</td>\n",
       "      <td>46.72281</td>\n",
       "      <td>72.166667</td>\n",
       "      <td>62.00</td>\n",
       "      <td>15.206666</td>\n",
       "      <td>74.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>18.44139</td>\n",
       "      <td>36.65</td>\n",
       "      <td>83.500</td>\n",
       "      <td>61.5</td>\n",
       "      <td>37.35861</td>\n",
       "      <td>74.333333</td>\n",
       "      <td>53.10</td>\n",
       "      <td>18.261984</td>\n",
       "      <td>74.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>21.39711</td>\n",
       "      <td>36.55</td>\n",
       "      <td>77.375</td>\n",
       "      <td>53.5</td>\n",
       "      <td>43.40289</td>\n",
       "      <td>60.833333</td>\n",
       "      <td>62.30</td>\n",
       "      <td>10.410812</td>\n",
       "      <td>74.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>20.37286</td>\n",
       "      <td>36.60</td>\n",
       "      <td>86.375</td>\n",
       "      <td>71.0</td>\n",
       "      <td>52.55214</td>\n",
       "      <td>73.833333</td>\n",
       "      <td>69.50</td>\n",
       "      <td>20.085223</td>\n",
       "      <td>82.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>24.54634</td>\n",
       "      <td>36.65</td>\n",
       "      <td>82.750</td>\n",
       "      <td>75.5</td>\n",
       "      <td>42.94116</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>65.10</td>\n",
       "      <td>18.943200</td>\n",
       "      <td>71.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>16.69572</td>\n",
       "      <td>36.55</td>\n",
       "      <td>88.875</td>\n",
       "      <td>64.0</td>\n",
       "      <td>61.60428</td>\n",
       "      <td>75.166667</td>\n",
       "      <td>76.80</td>\n",
       "      <td>19.039592</td>\n",
       "      <td>86.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>21.80115</td>\n",
       "      <td>36.85</td>\n",
       "      <td>82.750</td>\n",
       "      <td>68.5</td>\n",
       "      <td>46.54885</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>66.15</td>\n",
       "      <td>19.786841</td>\n",
       "      <td>74.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>27.74827</td>\n",
       "      <td>37.00</td>\n",
       "      <td>102.975</td>\n",
       "      <td>76.5</td>\n",
       "      <td>61.75173</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>85.70</td>\n",
       "      <td>24.909929</td>\n",
       "      <td>95.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>22.74039</td>\n",
       "      <td>36.90</td>\n",
       "      <td>81.750</td>\n",
       "      <td>70.0</td>\n",
       "      <td>45.38461</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>66.10</td>\n",
       "      <td>19.551547</td>\n",
       "      <td>71.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>25.11309</td>\n",
       "      <td>36.45</td>\n",
       "      <td>101.000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>66.13691</td>\n",
       "      <td>67.166667</td>\n",
       "      <td>87.20</td>\n",
       "      <td>23.138089</td>\n",
       "      <td>98.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>32.06292</td>\n",
       "      <td>37.10</td>\n",
       "      <td>80.625</td>\n",
       "      <td>63.5</td>\n",
       "      <td>38.08708</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>68.70</td>\n",
       "      <td>14.415127</td>\n",
       "      <td>79.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>25.31461</td>\n",
       "      <td>36.55</td>\n",
       "      <td>86.750</td>\n",
       "      <td>68.5</td>\n",
       "      <td>39.83539</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>60.85</td>\n",
       "      <td>24.854893</td>\n",
       "      <td>80.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>27.95181</td>\n",
       "      <td>36.95</td>\n",
       "      <td>89.500</td>\n",
       "      <td>68.5</td>\n",
       "      <td>40.49819</td>\n",
       "      <td>78.666667</td>\n",
       "      <td>64.15</td>\n",
       "      <td>23.653588</td>\n",
       "      <td>85.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>27.02738</td>\n",
       "      <td>36.90</td>\n",
       "      <td>88.000</td>\n",
       "      <td>64.5</td>\n",
       "      <td>46.04762</td>\n",
       "      <td>78.333333</td>\n",
       "      <td>70.70</td>\n",
       "      <td>18.641418</td>\n",
       "      <td>76.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>26.50830</td>\n",
       "      <td>36.50</td>\n",
       "      <td>89.000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>45.09170</td>\n",
       "      <td>69.166667</td>\n",
       "      <td>67.85</td>\n",
       "      <td>22.400611</td>\n",
       "      <td>77.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>28.04134</td>\n",
       "      <td>37.10</td>\n",
       "      <td>93.000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>44.74728</td>\n",
       "      <td>83.666667</td>\n",
       "      <td>72.80</td>\n",
       "      <td>22.815016</td>\n",
       "      <td>78.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0</td>\n",
       "      <td>15.40250</td>\n",
       "      <td>36.60</td>\n",
       "      <td>77.250</td>\n",
       "      <td>60.5</td>\n",
       "      <td>47.02250</td>\n",
       "      <td>83.166667</td>\n",
       "      <td>60.20</td>\n",
       "      <td>19.313673</td>\n",
       "      <td>72.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>18.86233</td>\n",
       "      <td>36.55</td>\n",
       "      <td>90.475</td>\n",
       "      <td>62.5</td>\n",
       "      <td>55.63767</td>\n",
       "      <td>77.666667</td>\n",
       "      <td>71.35</td>\n",
       "      <td>22.159841</td>\n",
       "      <td>86.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>24.49151</td>\n",
       "      <td>37.05</td>\n",
       "      <td>79.125</td>\n",
       "      <td>63.0</td>\n",
       "      <td>42.10849</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>63.20</td>\n",
       "      <td>20.972402</td>\n",
       "      <td>76.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>26.52392</td>\n",
       "      <td>36.35</td>\n",
       "      <td>94.475</td>\n",
       "      <td>75.0</td>\n",
       "      <td>44.87608</td>\n",
       "      <td>70.666667</td>\n",
       "      <td>70.00</td>\n",
       "      <td>24.982505</td>\n",
       "      <td>76.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>29.56324</td>\n",
       "      <td>36.40</td>\n",
       "      <td>101.875</td>\n",
       "      <td>60.0</td>\n",
       "      <td>63.23676</td>\n",
       "      <td>82.333333</td>\n",
       "      <td>90.70</td>\n",
       "      <td>26.586487</td>\n",
       "      <td>100.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>16.93165</td>\n",
       "      <td>36.60</td>\n",
       "      <td>79.375</td>\n",
       "      <td>72.5</td>\n",
       "      <td>47.29335</td>\n",
       "      <td>77.666667</td>\n",
       "      <td>62.05</td>\n",
       "      <td>21.782101</td>\n",
       "      <td>71.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>19.74082</td>\n",
       "      <td>36.80</td>\n",
       "      <td>89.200</td>\n",
       "      <td>51.0</td>\n",
       "      <td>61.20918</td>\n",
       "      <td>68.833333</td>\n",
       "      <td>78.65</td>\n",
       "      <td>18.768836</td>\n",
       "      <td>87.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>18.03599</td>\n",
       "      <td>36.65</td>\n",
       "      <td>85.500</td>\n",
       "      <td>54.5</td>\n",
       "      <td>56.66401</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>71.45</td>\n",
       "      <td>17.627779</td>\n",
       "      <td>83.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>19.21288</td>\n",
       "      <td>36.65</td>\n",
       "      <td>84.000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>40.58712</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>57.45</td>\n",
       "      <td>20.455248</td>\n",
       "      <td>76.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1</td>\n",
       "      <td>19.94891</td>\n",
       "      <td>37.20</td>\n",
       "      <td>89.600</td>\n",
       "      <td>70.5</td>\n",
       "      <td>54.02609</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>69.55</td>\n",
       "      <td>23.440393</td>\n",
       "      <td>89.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>21.99068</td>\n",
       "      <td>36.65</td>\n",
       "      <td>95.750</td>\n",
       "      <td>67.5</td>\n",
       "      <td>62.75932</td>\n",
       "      <td>76.333333</td>\n",
       "      <td>80.70</td>\n",
       "      <td>17.905916</td>\n",
       "      <td>94.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>21.98649</td>\n",
       "      <td>36.65</td>\n",
       "      <td>91.625</td>\n",
       "      <td>59.5</td>\n",
       "      <td>63.48851</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>82.90</td>\n",
       "      <td>15.925062</td>\n",
       "      <td>89.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>29.67915</td>\n",
       "      <td>36.80</td>\n",
       "      <td>95.000</td>\n",
       "      <td>58.5</td>\n",
       "      <td>49.67085</td>\n",
       "      <td>71.666667</td>\n",
       "      <td>76.90</td>\n",
       "      <td>17.512247</td>\n",
       "      <td>85.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>20.52818</td>\n",
       "      <td>36.75</td>\n",
       "      <td>76.500</td>\n",
       "      <td>54.5</td>\n",
       "      <td>42.39682</td>\n",
       "      <td>62.333333</td>\n",
       "      <td>60.00</td>\n",
       "      <td>14.067642</td>\n",
       "      <td>66.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>29.85703</td>\n",
       "      <td>36.60</td>\n",
       "      <td>86.125</td>\n",
       "      <td>65.0</td>\n",
       "      <td>39.79297</td>\n",
       "      <td>75.666667</td>\n",
       "      <td>65.65</td>\n",
       "      <td>20.294512</td>\n",
       "      <td>76.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>26.76094</td>\n",
       "      <td>36.70</td>\n",
       "      <td>98.875</td>\n",
       "      <td>73.5</td>\n",
       "      <td>62.21406</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>86.40</td>\n",
       "      <td>24.263134</td>\n",
       "      <td>94.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>24.87186</td>\n",
       "      <td>36.85</td>\n",
       "      <td>79.000</td>\n",
       "      <td>68.5</td>\n",
       "      <td>35.77814</td>\n",
       "      <td>63.833333</td>\n",
       "      <td>57.70</td>\n",
       "      <td>11.508113</td>\n",
       "      <td>68.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>19.12753</td>\n",
       "      <td>37.00</td>\n",
       "      <td>85.125</td>\n",
       "      <td>69.5</td>\n",
       "      <td>44.24747</td>\n",
       "      <td>70.333333</td>\n",
       "      <td>59.50</td>\n",
       "      <td>21.822562</td>\n",
       "      <td>76.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>18.60773</td>\n",
       "      <td>36.70</td>\n",
       "      <td>88.500</td>\n",
       "      <td>56.5</td>\n",
       "      <td>60.04227</td>\n",
       "      <td>78.666667</td>\n",
       "      <td>75.40</td>\n",
       "      <td>25.188640</td>\n",
       "      <td>86.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>29.92063</td>\n",
       "      <td>36.90</td>\n",
       "      <td>103.125</td>\n",
       "      <td>63.5</td>\n",
       "      <td>67.92937</td>\n",
       "      <td>67.833333</td>\n",
       "      <td>91.55</td>\n",
       "      <td>25.119208</td>\n",
       "      <td>95.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>29.53738</td>\n",
       "      <td>36.85</td>\n",
       "      <td>90.000</td>\n",
       "      <td>68.5</td>\n",
       "      <td>49.43711</td>\n",
       "      <td>87.833333</td>\n",
       "      <td>72.90</td>\n",
       "      <td>20.207282</td>\n",
       "      <td>82.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>20.81291</td>\n",
       "      <td>36.65</td>\n",
       "      <td>84.375</td>\n",
       "      <td>55.5</td>\n",
       "      <td>49.58709</td>\n",
       "      <td>75.666667</td>\n",
       "      <td>67.70</td>\n",
       "      <td>17.172831</td>\n",
       "      <td>75.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>22.18952</td>\n",
       "      <td>36.75</td>\n",
       "      <td>85.500</td>\n",
       "      <td>62.5</td>\n",
       "      <td>49.26048</td>\n",
       "      <td>71.833333</td>\n",
       "      <td>71.80</td>\n",
       "      <td>11.956105</td>\n",
       "      <td>78.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>20.35633</td>\n",
       "      <td>36.85</td>\n",
       "      <td>83.625</td>\n",
       "      <td>77.5</td>\n",
       "      <td>42.49367</td>\n",
       "      <td>69.166667</td>\n",
       "      <td>63.60</td>\n",
       "      <td>21.455928</td>\n",
       "      <td>76.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "      <td>26.71827</td>\n",
       "      <td>36.55</td>\n",
       "      <td>81.250</td>\n",
       "      <td>68.0</td>\n",
       "      <td>50.58173</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>74.30</td>\n",
       "      <td>19.863118</td>\n",
       "      <td>73.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>28.70265</td>\n",
       "      <td>36.55</td>\n",
       "      <td>88.375</td>\n",
       "      <td>52.0</td>\n",
       "      <td>36.49735</td>\n",
       "      <td>74.333333</td>\n",
       "      <td>64.20</td>\n",
       "      <td>15.434071</td>\n",
       "      <td>84.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GENDER       fma  B temp  B meanumb  B pulse      ffma  B meandbp  \\\n",
       "0         0  18.44968   36.80     82.500     64.5  33.20032  67.000000   \n",
       "1         0  25.62774   36.70     89.125     61.0  48.72226  64.166667   \n",
       "2         0  26.45387   36.60     79.750     73.0  41.09613  83.500000   \n",
       "3         0  30.86636   36.85     86.875     82.0  43.43364  74.833333   \n",
       "4         0  22.65525   36.70     80.625     65.0  41.19475  70.000000   \n",
       "5         0  28.67226   36.85     91.625     68.0  45.57774  65.166667   \n",
       "6         0  22.35399   36.55     87.375     67.5  45.39601  71.666667   \n",
       "7         1  16.03920   37.20     82.800     59.5  54.61080  72.000000   \n",
       "8         0  32.44627   36.75     84.375     79.0  39.75373  80.666667   \n",
       "9         0  16.93972   36.60     65.750     64.0  39.38528  76.500000   \n",
       "10        0  19.89859   36.55     82.250     54.0  45.15141  77.500000   \n",
       "11        0  20.75006   36.30     90.625     67.0  39.82494  66.666667   \n",
       "12        0  29.01078   36.85     86.375     73.0  44.93922  73.333333   \n",
       "13        1  19.23703   36.55     90.875     57.0  56.93797  66.666667   \n",
       "14        0  28.56019   36.85     91.250     61.0  44.23981  66.166667   \n",
       "15        1  23.34864   36.55     91.375     51.0  55.80136  71.166667   \n",
       "16        0  23.45835   37.15     85.125     60.5  40.49165  77.333333   \n",
       "17        0  22.77217   37.05     90.375     69.5  45.05283  66.833333   \n",
       "18        0  19.87719   36.85     79.000     61.5  46.72281  72.166667   \n",
       "19        0  18.44139   36.65     83.500     61.5  37.35861  74.333333   \n",
       "20        0  21.39711   36.55     77.375     53.5  43.40289  60.833333   \n",
       "21        1  20.37286   36.60     86.375     71.0  52.55214  73.833333   \n",
       "22        0  24.54634   36.65     82.750     75.5  42.94116  72.000000   \n",
       "23        1  16.69572   36.55     88.875     64.0  61.60428  75.166667   \n",
       "24        0  21.80115   36.85     82.750     68.5  46.54885  71.000000   \n",
       "25        1  27.74827   37.00    102.975     76.5  61.75173  81.000000   \n",
       "26        0  22.74039   36.90     81.750     70.0  45.38461  67.333333   \n",
       "27        1  25.11309   36.45    101.000     59.0  66.13691  67.166667   \n",
       "28        0  32.06292   37.10     80.625     63.5  38.08708  80.500000   \n",
       "29        0  25.31461   36.55     86.750     68.5  39.83539  83.333333   \n",
       "..      ...       ...     ...        ...      ...       ...        ...   \n",
       "77        0  27.95181   36.95     89.500     68.5  40.49819  78.666667   \n",
       "78        0  27.02738   36.90     88.000     64.5  46.04762  78.333333   \n",
       "79        0  26.50830   36.50     89.000     62.0  45.09170  69.166667   \n",
       "80        0  28.04134   37.10     93.000     75.0  44.74728  83.666667   \n",
       "81        0  15.40250   36.60     77.250     60.5  47.02250  83.166667   \n",
       "82        1  18.86233   36.55     90.475     62.5  55.63767  77.666667   \n",
       "83        0  24.49151   37.05     79.125     63.0  42.10849  75.000000   \n",
       "84        0  26.52392   36.35     94.475     75.0  44.87608  70.666667   \n",
       "85        1  29.56324   36.40    101.875     60.0  63.23676  82.333333   \n",
       "86        0  16.93165   36.60     79.375     72.5  47.29335  77.666667   \n",
       "87        1  19.74082   36.80     89.200     51.0  61.20918  68.833333   \n",
       "88        1  18.03599   36.65     85.500     54.5  56.66401  68.000000   \n",
       "89        0  19.21288   36.65     84.000     62.0  40.58712  62.000000   \n",
       "90        1  19.94891   37.20     89.600     70.5  54.02609  83.000000   \n",
       "91        1  21.99068   36.65     95.750     67.5  62.75932  76.333333   \n",
       "92        1  21.98649   36.65     91.625     59.5  63.48851  63.500000   \n",
       "93        0  29.67915   36.80     95.000     58.5  49.67085  71.666667   \n",
       "94        0  20.52818   36.75     76.500     54.5  42.39682  62.333333   \n",
       "95        0  29.85703   36.60     86.125     65.0  39.79297  75.666667   \n",
       "96        1  26.76094   36.70     98.875     73.5  62.21406  84.000000   \n",
       "97        0  24.87186   36.85     79.000     68.5  35.77814  63.833333   \n",
       "98        0  19.12753   37.00     85.125     69.5  44.24747  70.333333   \n",
       "99        1  18.60773   36.70     88.500     56.5  60.04227  78.666667   \n",
       "100       1  29.92063   36.90    103.125     63.5  67.92937  67.833333   \n",
       "101       0  29.53738   36.85     90.000     68.5  49.43711  87.833333   \n",
       "102       0  20.81291   36.65     84.375     55.5  49.58709  75.666667   \n",
       "103       0  22.18952   36.75     85.500     62.5  49.26048  71.833333   \n",
       "104       0  20.35633   36.85     83.625     77.5  42.49367  69.166667   \n",
       "105       0  26.71827   36.55     81.250     68.0  50.58173  62.500000   \n",
       "106       0  28.70265   36.55     88.375     52.0  36.49735  74.333333   \n",
       "\n",
       "     1 clinwt   AGE_WhtR  B meanwst  \n",
       "0       48.60  23.473256     72.250  \n",
       "1       71.60  20.252632     78.000  \n",
       "2       63.15  20.147369     75.875  \n",
       "3       71.70  14.007541     75.750  \n",
       "4       61.30  13.118728     75.875  \n",
       "5       71.70  24.361014     82.000  \n",
       "6       66.75  18.707309     78.125  \n",
       "7       66.30  17.772606     81.450  \n",
       "8       70.30  23.212059     80.875  \n",
       "9       52.80  16.568048     64.750  \n",
       "10      61.10  16.856245     74.625  \n",
       "11      58.20  22.262512     82.250  \n",
       "12      72.75  12.658328     74.000  \n",
       "13      71.90  23.081703     86.875  \n",
       "14      67.60  23.858505     85.500  \n",
       "15      75.90  23.592797     88.625  \n",
       "16      62.75  22.657431     76.875  \n",
       "17      65.15  22.654802     85.000  \n",
       "18      62.00  15.206666     74.750  \n",
       "19      53.10  18.261984     74.625  \n",
       "20      62.30  10.410812     74.000  \n",
       "21      69.50  20.085223     82.500  \n",
       "22      65.10  18.943200     71.100  \n",
       "23      76.80  19.039592     86.625  \n",
       "24      66.15  19.786841     74.375  \n",
       "25      85.70  24.909929     95.625  \n",
       "26      66.10  19.551547     71.100  \n",
       "27      87.20  23.138089     98.750  \n",
       "28      68.70  14.415127     79.250  \n",
       "29      60.85  24.854893     80.375  \n",
       "..        ...        ...        ...  \n",
       "77      64.15  23.653588     85.500  \n",
       "78      70.70  18.641418     76.250  \n",
       "79      67.85  22.400611     77.125  \n",
       "80      72.80  22.815016     78.600  \n",
       "81      60.20  19.313673     72.000  \n",
       "82      71.35  22.159841     86.200  \n",
       "83      63.20  20.972402     76.625  \n",
       "84      70.00  24.982505     76.700  \n",
       "85      90.70  26.586487    100.875  \n",
       "86      62.05  21.782101     71.000  \n",
       "87      78.65  18.768836     87.225  \n",
       "88      71.45  17.627779     83.375  \n",
       "89      57.45  20.455248     76.250  \n",
       "90      69.55  23.440393     89.050  \n",
       "91      80.70  17.905916     94.625  \n",
       "92      82.90  15.925062     89.875  \n",
       "93      76.90  17.512247     85.375  \n",
       "94      60.00  14.067642     66.250  \n",
       "95      65.65  20.294512     76.750  \n",
       "96      86.40  24.263134     94.000  \n",
       "97      57.70  11.508113     68.750  \n",
       "98      59.50  21.822562     76.875  \n",
       "99      75.40  25.188640     86.000  \n",
       "100     91.55  25.119208     95.375  \n",
       "101     72.90  20.207282     82.875  \n",
       "102     67.70  17.172831     75.875  \n",
       "103     71.80  11.956105     78.875  \n",
       "104     63.60  21.455928     76.250  \n",
       "105     74.30  19.863118     73.750  \n",
       "106     64.20  15.434071     84.125  \n",
       "\n",
       "[107 rows x 10 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare configuration for cross validation test harness\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare models\n",
    "models = []\n",
    "#models.append(('Lr1', LinearRegression()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('AB', AdaBoostClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.698182 (0.148813)\n",
      "LDA: 0.699091 (0.149790)\n",
      "KNN: 0.653636 (0.197021)\n",
      "CART: 0.681818 (0.165295)\n",
      "NB: 0.720000 (0.151728)\n",
      "SVM: 0.468182 (0.125128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\TENN\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: 0.692727 (0.089977)\n",
      "AB: 0.701818 (0.137017)\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X1, y1, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=10, random_state=7, shuffle=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('names', results)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = VotingClassifier(estimators, voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method VotingClassifier.fit of VotingClassifier(estimators=[('names',\n",
      "                              [array([0.63636364, 0.72727273, 0.63636364, 0.72727273, 0.63636364,\n",
      "       0.54545455, 0.63636364, 0.6       , 0.9       , 0.6       ]),\n",
      "                               array([0.63636364, 0.90909091, 0.81818182, 0.54545455, 0.63636364,\n",
      "       0.45454545, 0.54545455, 0.6       , 0.8       , 0.5       ]),\n",
      "                               array([0.36363636, 0.54545455, 0.72727273, 0.63636364, 0.45454545,\n",
      "       0.63636364, 0.72727273, 0.7       , 0.8       , 0.4       ]),\n",
      "                               array([0.45454545...\n",
      "                               array([0.36363636, 0.36363636, 0.54545455, 0.63636364, 0.54545455,\n",
      "       0.36363636, 0.45454545, 0.7       , 0.7       , 0.3       ]),\n",
      "                               array([0.54545455, 0.54545455, 0.63636364, 0.72727273, 0.72727273,\n",
      "       0.72727273, 0.63636364, 0.7       , 0.7       , 0.5       ]),\n",
      "                               array([0.36363636, 0.54545455, 0.63636364, 0.63636364, 0.72727273,\n",
      "       0.81818182, 0.63636364, 0.8       , 0.8       , 0.3       ])])],\n",
      "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
      "                 weights=None)>\n"
     ]
    }
   ],
   "source": [
    "print(ensemble.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object 'array([0.55555556, 0.52777778, 0.6       ])' (type <class 'numpy.ndarray'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-80d9d55a2fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\voting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     99\u001b[0m                 delayed(_parallel_fit_estimator)(clone(clf), X, y,\n\u001b[0;32m    100\u001b[0m                                                  sample_weight=sample_weight)\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclfs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'drop'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             )\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    752\u001b[0m             tasks = BatchedCalls(itertools.islice(iterator, batch_size),\n\u001b[0;32m    753\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_nested_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m                                  self._pickle_cache)\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m                 \u001b[1;31m# No more tasks available in the iterator: tell caller to stop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, iterator_slice, backend_and_jobs, pickle_cache)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator_slice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend_and_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator_slice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend_and_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\voting.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     99\u001b[0m                 delayed(_parallel_fit_estimator)(clone(clf), X, y,\n\u001b[0;32m    100\u001b[0m                                                  sample_weight=sample_weight)\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclfs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'drop'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m             )\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# XXX: not handling dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_params'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# XXX: not handling dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_params'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     58\u001b[0m                             \u001b[1;34m\"it does not seem to be a scikit-learn estimator \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                             \u001b[1;34m\"as it does not implement a 'get_params' methods.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                             % (repr(estimator), type(estimator)))\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot clone object 'array([0.55555556, 0.52777778, 0.6       ])' (type <class 'numpy.ndarray'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods."
     ]
    }
   ],
   "source": [
    "ensemble.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
